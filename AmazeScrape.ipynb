{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b99e6fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "#import dataset\n",
    "from bs4 import BeautifulSoup \n",
    "import requests\n",
    "from urllib.parse import unquote\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80c638c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiating the session\n",
    "scrape = requests.Session()\n",
    "#Getting the first URL\n",
    "entry = 'https://www.amazon.in/'"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "04f3a3a1",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8f73fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = scrape.get(entry)\n",
    "soup_main = BeautifulSoup(e.text,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4de1cc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the links\n",
    "main_link = soup_main.find_all('a',class_='nav-a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb22aec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "links=[]\n",
    "for main in main_link:\n",
    "    links.append(main.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bf76721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are total 33 links collected from amazon main page\n"
     ]
    }
   ],
   "source": [
    "#select those links that start with '/'\n",
    "collected = []\n",
    "for link in links[3:]:\n",
    "    temp = urljoin(entry,link)\n",
    "    collected.append(temp)\n",
    "        \n",
    "print('There are total {} links collected from amazon main page'.format(len(collected)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "786b0422",
   "metadata": {},
   "outputs": [],
   "source": [
    "#There is a pattern to the links. End of every link is having the clue to where the link is heading.\n",
    "main_dict = {}\n",
    "for links in collected:\n",
    "    temp = links.split('=')\n",
    "    #split the link, take the heading\n",
    "    main_dict[temp[-1]] = links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2400fd83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['nav_orders_first', 'nav_cart', 'nav_cs_bestsellers', 'nav_cs_mobiles', 'nav_cs_gb', 'nav_cs_help', 'nav_cs_electronics', 'nav_cs_primelink_nonmember', 'nav_cs_books', 'nav_cs_fashion', 'nav_cs_newreleases', 'nav_cs_home', 'nav_cs_apay', 'nav_cs_pc', 'nav_cs_coupons', 'nav_cs_toys', 'nav_cs_sell', 'nav_cs_gc', 'nav_cs_automotive', 'nav_cs_beauty', 'nav_cs_grocery', 'nav_cs_video_games', 'nav_cs_sports', 'nav_cs_hpc', 'nav_cs_baby', 'nav_cs_pets', 'nav_cs_kindle_books', 'nav_cs_hi', 'nav_cs_giftfinder', 'nav_cs_amazonbasics', 'nav_cs_audible', 'nav_cs_sns'])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6db4988c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.amazon.in/gp/sva/dashboard?ref_=nav_cs_apay'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#There are couple of links that lead to personal areas, that require sign-in. Poping them from the dict\n",
    "main_dict.pop('nav_orders_first')\n",
    "main_dict.pop('nav_cart')\n",
    "main_dict.pop('nav_cs_primelink_nonmember')\n",
    "main_dict.pop('nav_cs_help')\n",
    "main_dict.pop('nav_cs_sns')\n",
    "main_dict.pop('nav_cs_apay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c2090a28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#making dataframe from the dictionary of links\n",
    "link_collector = pd.DataFrame(list(main_dict.items()),columns=['topics','links'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "15bbf90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating headers so the Amazon website can be accessed\n",
    "my_headers = {\n",
    "'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 ' + ' (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36',\n",
    "'referer':'http://localhost:8888/'    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fbbd85ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.amazon.in/gp/bestsellers/?ref_=nav_cs_bestsellers'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link_collector.links[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "16d835a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The below routine fails since the headers don't work with Amazon correctly. So not very useful, \n",
    "link_collector_dict = {} #dict to hold the list of links collected from each links\n",
    "\n",
    "\n",
    "for index, crawl in enumerate(link_collector.links):\n",
    "    link_hrf = []\n",
    "    #print(index,crawl)\n",
    "    lev = scrape.get(crawl,headers=my_headers)\n",
    "    lev_soup = BeautifulSoup(lev.text,'html.parser')\n",
    "    for hrf in lev_soup.find_all('a', attrs={'class':'a-link-normal'}):     \n",
    "        temp = hrf.get('href')\n",
    "        link_hrf.append(temp)\n",
    "\n",
    "    link_collector_dict[link_collector.topics[index]] = link_hrf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a10bfcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "link_list_df = pd.DataFrame(list(link_collector_dict.items()),columns=['topics','links_list'])\n",
    "link_list_df['link_len'] = link_list_df.links_list.apply(lambda x: len(x))\n",
    "link_list_df = link_list_df[link_list_df.link_len > 60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "dc74c053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topics</th>\n",
       "      <th>links_list</th>\n",
       "      <th>link_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nav_cs_bestsellers</td>\n",
       "      <td>[/gp/bestsellers/luggage, /Priority-Disney-Pri...</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nav_cs_newreleases</td>\n",
       "      <td>[/gp/new-releases/luggage, /WILDHORN-Leather-S...</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               topics                                         links_list  \\\n",
       "0  nav_cs_bestsellers  [/gp/bestsellers/luggage, /Priority-Disney-Pri...   \n",
       "6  nav_cs_newreleases  [/gp/new-releases/luggage, /WILDHORN-Leather-S...   \n",
       "\n",
       "   link_len  \n",
       "0       146  \n",
       "6       144  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link_list_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "82a922d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building link from the pages\n",
    "def build_link(links_list):\n",
    "    main_link = []\n",
    "    for links in links_list:\n",
    "        if links[:3] == '/gp':\n",
    "            x = urljoin(entry,links)\n",
    "            main_link.append(x)\n",
    "    return main_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9ca92f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_release = build_link(link_list_df.links_list[6])\n",
    "best_sellers = build_link(link_list_df.links_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f21cd625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.amazon.in/gp/new-releases/luggage',\n",
       " 'https://www.amazon.in/gp/new-releases/garden',\n",
       " 'https://www.amazon.in/gp/new-releases/sports',\n",
       " 'https://www.amazon.in/gp/new-releases/videogames',\n",
       " 'https://www.amazon.in/gp/new-releases/electronics',\n",
       " 'https://www.amazon.in/gp/new-releases/office']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_release[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37658581",
   "metadata": {},
   "source": [
    "Out of 28 links only 3 pages loaded with default referer and settings. Rest of page needs more info. \n",
    "I am planning to go ahead scrape the links under bestsellers first\n",
    "\n",
    "1) Start by making the list of links under best sellers as full links and see if they provide the content\n",
    "\n",
    "2) If it provides the content, then check the tags, class, id that can be used to get the data.\n",
    "\n",
    "3) Create the dataframe and store the data directly in the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c7c799",
   "metadata": {},
   "source": [
    "learn more about the usage\n",
    "\n",
    "driver.execute_script(\"window.scrollBy(0, document.body.scrollHeight-400)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0ae76629",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiating the Selenium Drivers\n",
    "from selenium import webdriver\n",
    "import time as time\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import ElementNotInteractableException\n",
    "from selenium.webdriver.chrome.options import Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b983162e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pagelevel_data(beauty_soup, df):\n",
    "    trial_L1 = beauty_soup.find_all('div',attrs={'id':'gridItemRoot'})\n",
    "    start = len(df)\n",
    "    for index,trial in enumerate(trial_L1):\n",
    "        df.loc[start+index,'Name'] = trial.select('span')[1].get_text()\n",
    "        df.loc[start+index,'reviews'] = trial.select('span')[3].get_text()\n",
    "        df.loc[start+index,'stars'] = trial.select('span')[2].get_text()\n",
    "        df.loc[start+index,'price'] = trial.select('span')[4].get_text()\n",
    "        df.loc[start+index,'link_in'] = trial.find('a',attrs={'class':'a-link-normal'}).get('href')\n",
    "    end = len(df)\n",
    "    print('Total data got is {}'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a5eea481",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a subroutine to get page\n",
    "def get_page(main_driver):\n",
    "    element = main_driver.find_element_by_class_name('a-last')\n",
    "    main_driver.execute_script(\"arguments[0].scrollIntoView();\",element)\n",
    "    time.sleep(5)\n",
    "    soup = main_driver.page_source\n",
    "    trial = BeautifulSoup(soup,'html.parser')\n",
    "    return trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "05256ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the pages that are still un-selected. These many clicks will be required to traverse the site\n",
    "def get_page_avbl(page_soup):\n",
    "    return len(page_soup.find_all('li',attrs={'class':'a-normal'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "09d00e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawler(page_url):\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(page_url)\n",
    "    driver.maximize_window()\n",
    "    stor_dataframe = pd.DataFrame()\n",
    "    #Above code initialises the selenium driver and opens the browser\n",
    "    \n",
    "    firstpage = get_page(driver) #gets the first page\n",
    "    get_pagelevel_data(firstpage,stor_dataframe) # gets the data in the first page\n",
    "    page_avbl = get_page_avbl(firstpage) #gets the number of pages\n",
    "    print('This url contains {} pages'.format(page_avbl+1))\n",
    "    for page in range(page_avbl):#looping the pages available\n",
    "        print('Moving to {} page for scraping'.format(page+2))\n",
    "        try:\n",
    "            driver.find_element_by_class_name('a-last').click() #moving to the next page\n",
    "            page_next = get_page(driver) #getting next page data\n",
    "            get_pagelevel_data(page_next,stor_dataframe) # collecting the next page data\n",
    "        except NoSuchElementException as nse:\n",
    "            print('Element not found, breaking')\n",
    "    print('ending crawler for {} and returning payload'.format(page_url))\n",
    "    print('Links collected is {} and quitting driver'.format(stor_dataframe.shape[0]))\n",
    "    driver.quit()\n",
    "    return stor_dataframe #returning the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0e252bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Even though the crawler is headless, the printing messages will feedback on the status\n",
    "def headless_crawler(page_url):\n",
    "    options = Options()\n",
    "    options.headless = True #making this driver headless so it can work in the background\n",
    "    driver = webdriver.Chrome(options=options, executable_path=r'chromedriver.exe')\n",
    "    driver.get(page_url)\n",
    "    driver.maximize_window()\n",
    "    stor_dataframe = pd.DataFrame()\n",
    "    #Above code initialises the selenium driver and opens the browser\n",
    "    \n",
    "    firstpage = get_page(driver) #gets the first page\n",
    "    get_pagelevel_data(firstpage,stor_dataframe) # gets the data in the first page\n",
    "    page_avbl = get_page_avbl(firstpage) #gets the number of pages\n",
    "    print('This url contains {} pages'.format(page_avbl+1))\n",
    "    for page in range(page_avbl):#looping the pages available\n",
    "        print('Moving to {} page for scraping'.format(page+2))\n",
    "        try:\n",
    "            driver.find_element_by_class_name('a-last').click() #moving to the next page\n",
    "            page_next = get_page(driver) #getting next page data\n",
    "            get_pagelevel_data(page_next,stor_dataframe) # collecting the next page data\n",
    "        except NoSuchElementException as nse:\n",
    "            print('Element not found, breaking')\n",
    "    print('ending crawler for {} and returning payload'.format(page_url))\n",
    "    print('Links collected is {} and quitting driver'.format(stor_dataframe.shape[0]))\n",
    "    driver.quit()\n",
    "    return stor_dataframe #returning the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "89108247",
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_url = 'https://www.amazon.in/gp/bestsellers/videogames'\n",
    "bag_url = 'https://www.amazon.in/gp/bestsellers/luggage'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ee838379",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\WorkFiles\\WPy64-37120\\pypy3.7-v7.3.7-win64\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  \"\"\"\n",
      "C:\\WorkFiles\\WPy64-37120\\pypy3.7-v7.3.7-win64\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data got is 50\n",
      "This url contains 2 pages\n",
      "Moving to 2 page for scraping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\WorkFiles\\WPy64-37120\\pypy3.7-v7.3.7-win64\\site-packages\\ipykernel_launcher.py:18: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data got is 50\n",
      "ending crawler for https://www.amazon.in/gp/bestsellers/videogames and returning payload\n",
      "Links collected is 100 and quitting driver\n"
     ]
    }
   ],
   "source": [
    "#The headless works.... :D\n",
    "vid_games = headless_crawler(vid_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1ee1f8b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>reviews</th>\n",
       "      <th>stars</th>\n",
       "      <th>price</th>\n",
       "      <th>link_in</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AmazonBasics Extended Gaming Mouse Pad,Black</td>\n",
       "      <td>35,808</td>\n",
       "      <td>4.5 out of 5 stars</td>\n",
       "      <td>₹469.00</td>\n",
       "      <td>/AmazonBasics-Extended-Gaming-Mouse-Black/dp/B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rs.1000 Sony PlayStation Network Wallet Top-Up...</td>\n",
       "      <td>4.7 out of 5 stars</td>\n",
       "      <td>Sony Interactive Entertainment Europe</td>\n",
       "      <td>3,115</td>\n",
       "      <td>/Rs-1000-Sony-PlayStation-Network-Delivery/dp/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rs.500 Sony PlayStation Network Wallet Top-Up ...</td>\n",
       "      <td>4.7 out of 5 stars</td>\n",
       "      <td>Sony Interactive Entertainment Europe</td>\n",
       "      <td>3,107</td>\n",
       "      <td>/Sony-PlayStation-Network-Delivery-Digital/dp/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JBL Quantum 100, Wired Over Ear Gaming Headpho...</td>\n",
       "      <td>3,473</td>\n",
       "      <td>4.0 out of 5 stars</td>\n",
       "      <td>₹2,299.00</td>\n",
       "      <td>/JBL-Quantum-Over-Ear-Headset-Detachable/dp/B0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cosmic Byte GS410 Wired Over-ear Headphones wi...</td>\n",
       "      <td>4.0 out of 5 stars</td>\n",
       "      <td>Cosmic Byte</td>\n",
       "      <td>18,977</td>\n",
       "      <td>/Cosmic-Byte-Headphones-Laptop-Android/dp/B074...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Name             reviews  \\\n",
       "0       AmazonBasics Extended Gaming Mouse Pad,Black              35,808   \n",
       "1  Rs.1000 Sony PlayStation Network Wallet Top-Up...  4.7 out of 5 stars   \n",
       "2  Rs.500 Sony PlayStation Network Wallet Top-Up ...  4.7 out of 5 stars   \n",
       "3  JBL Quantum 100, Wired Over Ear Gaming Headpho...               3,473   \n",
       "4  Cosmic Byte GS410 Wired Over-ear Headphones wi...  4.0 out of 5 stars   \n",
       "\n",
       "                                   stars      price  \\\n",
       "0                     4.5 out of 5 stars    ₹469.00   \n",
       "1  Sony Interactive Entertainment Europe      3,115   \n",
       "2  Sony Interactive Entertainment Europe      3,107   \n",
       "3                     4.0 out of 5 stars  ₹2,299.00   \n",
       "4                            Cosmic Byte     18,977   \n",
       "\n",
       "                                             link_in  \n",
       "0  /AmazonBasics-Extended-Gaming-Mouse-Black/dp/B...  \n",
       "1  /Rs-1000-Sony-PlayStation-Network-Delivery/dp/...  \n",
       "2  /Sony-PlayStation-Network-Delivery-Digital/dp/...  \n",
       "3  /JBL-Quantum-Over-Ear-Headset-Detachable/dp/B0...  \n",
       "4  /Cosmic-Byte-Headphones-Laptop-Android/dp/B074...  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vid_games.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a31d193",
   "metadata": {},
   "source": [
    "Next step is to join the URLs and get the product level details. \n",
    "\n",
    "Join the link of each product, by creating the function for that purpose and send the created dataframe\n",
    "\n",
    "scrape the product page of the information\n",
    "\n",
    "store it back in the initial dataframe itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "939d746b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating full_linkins for products\n",
    "def f_link_create(df):\n",
    "    for index,links in enumerate(df.link_in):\n",
    "        #print(links)\n",
    "        df.loc[index,'f_link'] = urljoin('https://www.amazon.in/',links)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ccb714ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_games = f_link_create(vid_games)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6a84fa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def headless_crawler_product(page_url,df,indexF):\n",
    "    options = Options()\n",
    "    options.headless = True #making this driver headless so it can work in the background\n",
    "    driver = webdriver.Chrome(options=options, executable_path=r'chromedriver.exe')\n",
    "    driver.get(page_url)\n",
    "    driver.maximize_window()\n",
    "    prod_soup = driver.page_source\n",
    "    prod_nugs = BeautifulSoup(prod_soup,'html.parser')\n",
    "   \n",
    "    #Populating the dataframe directly with the below commands. The details were found using \n",
    "    table_prod_1 = prod_nugs.find('table',attrs={'id':'productDetails_techSpec_section_1'})\n",
    "    table_prod_2 = prod_nugs.find('table',attrs={'id':'productDetails_detailBullets_sections1'})\n",
    "    review_table = prod_nugs.find_all('table',attrs={'id':'histogramTable'})[1]\n",
    "    if table_prod_1 == None or table_prod_2 == None :\n",
    "        review_table = prod_nugs.find_all('table',attrs={'id':'histogramTable'})[1]\n",
    "            #Scraping of the data from the tables\n",
    "        df.loc[indexF,'brand'] = 'voucher_or_program'\n",
    "        df.loc[indexF,'color'] = 'voucher_or_program'\n",
    "        df.loc[indexF,'Pdt_dimension'] = 'voucher_or_program'\n",
    "        df.loc[indexF,'Weight'] = 'voucher_or_program'\n",
    "        df.loc[indexF,'origin'] = 'voucher_or_program'\n",
    "        df.loc[indexF,'ASIN'] = 'voucher_or_program'\n",
    "        df.loc[indexF,'category'] = 'voucher_or_program'\n",
    "        df.loc[indexF,'5Star'] = review_table.find_all('tr')[0].find_all('span',attrs={'class':'a-size-base'})[1].get_text().strip()\n",
    "        df.loc[indexF,'4Star'] = review_table.find_all('tr')[1].find_all('span',attrs={'class':'a-size-base'})[1].get_text().strip()\n",
    "        df.loc[indexF,'3Star'] = review_table.find_all('tr')[2].find_all('span',attrs={'class':'a-size-base'})[1].get_text().strip()\n",
    "        df.loc[indexF,'2Star'] = review_table.find_all('tr')[3].find_all('span',attrs={'class':'a-size-base'})[1].get_text().strip()\n",
    "        df.loc[indexF,'1Star'] = review_table.find_all('tr')[4].find_all('span',attrs={'class':'a-size-base'})[1].get_text().strip()\n",
    "    else:\n",
    "        #Scraping of the data from the tables\n",
    "        df.loc[indexF,'brand'] = table_prod_1.find_all('td')[0].get_text().strip()\n",
    "        df.loc[indexF,'color'] = table_prod_1.find_all('td')[2].get_text().strip()\n",
    "        df.loc[indexF,'Pdt_dimension'] = table_prod_2.find_all('td')[-2].get_text().strip()\n",
    "        df.loc[indexF,'Weight'] = table_prod_2.find_all('td')[-1].get_text().strip()\n",
    "        df.loc[indexF,'origin'] = table_prod_1.find_all('td')[-1].get_text().strip()\n",
    "        df.loc[indexF,'ASIN'] = table_prod_2.find_all('td')[0].get_text().strip()\n",
    "        df.loc[indexF,'category'] = table_prod_2.find_all('td')[-1].get_text().strip()\n",
    "        df.loc[indexF,'5Star'] = review_table.find_all('tr')[0].find_all('span',attrs={'class':'a-size-base'})[1].get_text().strip()\n",
    "        df.loc[indexF,'4Star'] = review_table.find_all('tr')[1].find_all('span',attrs={'class':'a-size-base'})[1].get_text().strip()\n",
    "        df.loc[indexF,'3Star'] = review_table.find_all('tr')[2].find_all('span',attrs={'class':'a-size-base'})[1].get_text().strip()\n",
    "        df.loc[indexF,'2Star'] = review_table.find_all('tr')[3].find_all('span',attrs={'class':'a-size-base'})[1].get_text().strip()\n",
    "        df.loc[indexF,'1Star'] = review_table.find_all('tr')[4].find_all('span',attrs={'class':'a-size-base'})[1].get_text().strip()\n",
    "    \n",
    "    print('Completed the data collection')\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "140faeaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\WorkFiles\\WPy64-37120\\pypy3.7-v7.3.7-win64\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n"
     ]
    }
   ],
   "source": [
    "for indexF,prod_link in enumerate(vid_games.f_link):\n",
    "    headless_crawler_product(prod_link,vid_games,indexF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "95d76b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_games.to_csv('Amazon_VG.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e8d481bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\WorkFiles\\WPy64-37120\\pypy3.7-v7.3.7-win64\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  \"\"\"\n",
      "C:\\WorkFiles\\WPy64-37120\\pypy3.7-v7.3.7-win64\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data got is 50\n",
      "This url contains 2 pages\n",
      "Moving to 2 page for scraping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\WorkFiles\\WPy64-37120\\pypy3.7-v7.3.7-win64\\site-packages\\ipykernel_launcher.py:18: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data got is 50\n",
      "ending crawler for https://www.amazon.in/gp/bestsellers/luggage and returning payload\n",
      "Links collected is 100 and quitting driver\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\WorkFiles\\WPy64-37120\\pypy3.7-v7.3.7-win64\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n",
      "Completed the data collection\n"
     ]
    }
   ],
   "source": [
    "bag_data = headless_crawler(bag_url)\n",
    "bag_data = f_link_create(bag_data)\n",
    "for indexF,prod_link in enumerate(bag_data.f_link):\n",
    "    headless_crawler_product(prod_link,bag_data,indexF)\n",
    "bag_data.to_csv('Amazon_bag.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def5a5eb",
   "metadata": {},
   "source": [
    "### Next Steps:\n",
    "\n",
    "The products that are newly released are having a different page profile, so a different function to be written. \n",
    "    Get_newRelases()\n",
    "\n",
    "Products are sold by different sellers, and branded by different companies. Get the information on the sellers, companies, brands. First the links needs to be retrived, then built into full links to access the source links \n",
    "\n",
    "    get_biz_Org()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0447e0cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd7b910",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9413833e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff65bc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f01ec92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99568c6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29312ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
